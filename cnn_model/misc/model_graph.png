digraph {
	graph [size="131.25,131.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2475465089872 [label="
 (1, 1, 384, 384)" fillcolor=darkolivegreen1]
	2475464651344 [label=ConvolutionBackward0]
	2475464651104 -> 2475464651344
	2475464651104 [label=ReluBackward0]
	2475464651584 -> 2475464651104
	2475464651584 [label=NativeBatchNormBackward0]
	2475464651440 -> 2475464651584
	2475464651440 [label=ConvolutionBackward0]
	2475464651776 -> 2475464651440
	2475464651776 [label=ReluBackward0]
	2475464651968 -> 2475464651776
	2475464651968 [label=NativeBatchNormBackward0]
	2475464652304 -> 2475464651968
	2475464652304 [label=ConvolutionBackward0]
	2475464652112 -> 2475464652304
	2475464652112 [label=ReluBackward0]
	2475464652448 -> 2475464652112
	2475464652448 [label=NativeBatchNormBackward0]
	2475464652544 -> 2475464652448
	2475464652544 [label=ConvolutionBackward0]
	2475464652736 -> 2475464652544
	2475464652736 [label=ReluBackward0]
	2475464652928 -> 2475464652736
	2475464652928 [label=NativeBatchNormBackward0]
	2475464653024 -> 2475464652928
	2475464653024 [label=ConvolutionBackward0]
	2475464653216 -> 2475464653024
	2475464653216 [label=ReluBackward0]
	2475464653408 -> 2475464653216
	2475464653408 [label=NativeBatchNormBackward0]
	2475464653504 -> 2475464653408
	2475464653504 [label=ConvolutionBackward0]
	2475464653696 -> 2475464653504
	2475464653696 [label=CatBackward0]
	2475462645824 -> 2475464653696
	2475462645824 [label=UpsampleNearest2DBackward0]
	2475464654080 -> 2475462645824
	2475464654080 [label=ReluBackward0]
	2475464654176 -> 2475464654080
	2475464654176 [label=NativeBatchNormBackward0]
	2475464654272 -> 2475464654176
	2475464654272 [label=ConvolutionBackward0]
	2475464654464 -> 2475464654272
	2475464654464 [label=ReluBackward0]
	2475464654656 -> 2475464654464
	2475464654656 [label=NativeBatchNormBackward0]
	2475464654752 -> 2475464654656
	2475464654752 [label=ConvolutionBackward0]
	2475465179296 -> 2475464654752
	2475465179296 [label=ReluBackward0]
	2475465179488 -> 2475465179296
	2475465179488 [label=NativeBatchNormBackward0]
	2475465179584 -> 2475465179488
	2475465179584 [label=ConvolutionBackward0]
	2475465179776 -> 2475465179584
	2475465179776 [label=ReluBackward0]
	2475465179968 -> 2475465179776
	2475465179968 [label=NativeBatchNormBackward0]
	2475465180064 -> 2475465179968
	2475465180064 [label=ConvolutionBackward0]
	2475465180256 -> 2475465180064
	2475465180256 [label=AddBackward0]
	2475465180448 -> 2475465180256
	2475465180448 [label=ReluBackward0]
	2475465180592 -> 2475465180448
	2475465180592 [label=NativeBatchNormBackward0]
	2475465180688 -> 2475465180592
	2475465180688 [label=ConvolutionBackward0]
	2475465180880 -> 2475465180688
	2475465180880 [label=ReluBackward0]
	2475465181072 -> 2475465180880
	2475465181072 [label=NativeBatchNormBackward0]
	2475465181168 -> 2475465181072
	2475465181168 [label=ConvolutionBackward0]
	2475465181360 -> 2475465181168
	2475465181360 [label=ReluBackward0]
	2475465181552 -> 2475465181360
	2475465181552 [label=NativeBatchNormBackward0]
	2475465181648 -> 2475465181552
	2475465181648 [label=ConvolutionBackward0]
	2475465181840 -> 2475465181648
	2475465181840 [label=AddBackward0]
	2475465182032 -> 2475465181840
	2475465182032 [label=ReluBackward0]
	2475465182176 -> 2475465182032
	2475465182176 [label=NativeBatchNormBackward0]
	2475465182272 -> 2475465182176
	2475465182272 [label=ConvolutionBackward0]
	2475465182464 -> 2475465182272
	2475465182464 [label=ReluBackward0]
	2475465182656 -> 2475465182464
	2475465182656 [label=NativeBatchNormBackward0]
	2475465182752 -> 2475465182656
	2475465182752 [label=ConvolutionBackward0]
	2475465182944 -> 2475465182752
	2475465182944 [label=ReluBackward0]
	2475465183136 -> 2475465182944
	2475465183136 [label=NativeBatchNormBackward0]
	2475465183232 -> 2475465183136
	2475465183232 [label=ConvolutionBackward0]
	2475465183424 -> 2475465183232
	2475465183424 [label=AddBackward0]
	2475465183616 -> 2475465183424
	2475465183616 [label=ReluBackward0]
	2475465183760 -> 2475465183616
	2475465183760 [label=NativeBatchNormBackward0]
	2475465183856 -> 2475465183760
	2475465183856 [label=ConvolutionBackward0]
	2475465184048 -> 2475465183856
	2475465184048 [label=ReluBackward0]
	2475465184240 -> 2475465184048
	2475465184240 [label=NativeBatchNormBackward0]
	2475465184336 -> 2475465184240
	2475465184336 [label=ConvolutionBackward0]
	2475465184528 -> 2475465184336
	2475465184528 [label=ReluBackward0]
	2475465184720 -> 2475465184528
	2475465184720 [label=NativeBatchNormBackward0]
	2475465184816 -> 2475465184720
	2475465184816 [label=ConvolutionBackward0]
	2475465185008 -> 2475465184816
	2475465185008 [label=ReluBackward0]
	2475465185200 -> 2475465185008
	2475465185200 [label=AddBackward0]
	2475465185296 -> 2475465185200
	2475465185296 [label=NativeBatchNormBackward0]
	2475465185440 -> 2475465185296
	2475465185440 [label=ConvolutionBackward0]
	2475465185632 -> 2475465185440
	2475465185632 [label=ReluBackward0]
	2475465185776 -> 2475465185632
	2475465185776 [label=NativeBatchNormBackward0]
	2475465185872 -> 2475465185776
	2475465185872 [label=ConvolutionBackward0]
	2475465185248 -> 2475465185872
	2475465185248 [label=ReluBackward0]
	2475465186160 -> 2475465185248
	2475465186160 [label=AddBackward0]
	2475465186256 -> 2475465186160
	2475465186256 [label=NativeBatchNormBackward0]
	2475465186400 -> 2475465186256
	2475465186400 [label=ConvolutionBackward0]
	2475465186592 -> 2475465186400
	2475465186592 [label=ReluBackward0]
	2475465186736 -> 2475465186592
	2475465186736 [label=NativeBatchNormBackward0]
	2475465186832 -> 2475465186736
	2475465186832 [label=ConvolutionBackward0]
	2475465183568 -> 2475465186832
	2475465183568 [label=ReluBackward0]
	2475465187120 -> 2475465183568
	2475465187120 [label=AddBackward0]
	2475465187168 -> 2475465187120
	2475465187168 [label=NativeBatchNormBackward0]
	2475465187408 -> 2475465187168
	2475465187408 [label=ConvolutionBackward0]
	2475465187600 -> 2475465187408
	2475465187600 [label=ReluBackward0]
	2475465187744 -> 2475465187600
	2475465187744 [label=NativeBatchNormBackward0]
	2475465187792 -> 2475465187744
	2475465187792 [label=ConvolutionBackward0]
	2475465186928 -> 2475465187792
	2475465186928 [label=ReluBackward0]
	2475465188176 -> 2475465186928
	2475465188176 [label=AddBackward0]
	2475465188224 -> 2475465188176
	2475465188224 [label=NativeBatchNormBackward0]
	2475465188464 -> 2475465188224
	2475465188464 [label=ConvolutionBackward0]
	2475465188656 -> 2475465188464
	2475465188656 [label=ReluBackward0]
	2475465188800 -> 2475465188656
	2475465188800 [label=NativeBatchNormBackward0]
	2475465188848 -> 2475465188800
	2475465188848 [label=ConvolutionBackward0]
	2475465181984 -> 2475465188848
	2475465181984 [label=ReluBackward0]
	2475465189232 -> 2475465181984
	2475465189232 [label=AddBackward0]
	2475465189280 -> 2475465189232
	2475465189280 [label=NativeBatchNormBackward0]
	2475465189520 -> 2475465189280
	2475465189520 [label=ConvolutionBackward0]
	2475465189712 -> 2475465189520
	2475465189712 [label=ReluBackward0]
	2475465189856 -> 2475465189712
	2475465189856 [label=NativeBatchNormBackward0]
	2475465189904 -> 2475465189856
	2475465189904 [label=ConvolutionBackward0]
	2475465189040 -> 2475465189904
	2475465189040 [label=ReluBackward0]
	2475465190288 -> 2475465189040
	2475465190288 [label=AddBackward0]
	2475465190336 -> 2475465190288
	2475465190336 [label=NativeBatchNormBackward0]
	2475465190576 -> 2475465190336
	2475465190576 [label=ConvolutionBackward0]
	2475465190768 -> 2475465190576
	2475465190768 [label=ReluBackward0]
	2475465190912 -> 2475465190768
	2475465190912 [label=NativeBatchNormBackward0]
	2475465190960 -> 2475465190912
	2475465190960 [label=ConvolutionBackward0]
	2475465180400 -> 2475465190960
	2475465180400 [label=ReluBackward0]
	2475465191344 -> 2475465180400
	2475465191344 [label=AddBackward0]
	2475465191392 -> 2475465191344
	2475465191392 [label=NativeBatchNormBackward0]
	2475465191632 -> 2475465191392
	2475465191632 [label=ConvolutionBackward0]
	2475465191824 -> 2475465191632
	2475465191824 [label=ReluBackward0]
	2475465191968 -> 2475465191824
	2475465191968 [label=NativeBatchNormBackward0]
	2475465192016 -> 2475465191968
	2475465192016 [label=ConvolutionBackward0]
	2475465191152 -> 2475465192016
	2475465191152 [label=ReluBackward0]
	2475465192400 -> 2475465191152
	2475465192400 [label=AddBackward0]
	2475465192448 -> 2475465192400
	2475465192448 [label=NativeBatchNormBackward0]
	2475465192688 -> 2475465192448
	2475465192688 [label=ConvolutionBackward0]
	2475465192880 -> 2475465192688
	2475465192880 [label=ReluBackward0]
	2475465193024 -> 2475465192880
	2475465193024 [label=NativeBatchNormBackward0]
	2475465193072 -> 2475465193024
	2475465193072 [label=ConvolutionBackward0]
	2475465192208 -> 2475465193072
	2475465192208 [label=MaxPool2DWithIndicesBackward0]
	2475465193456 -> 2475465192208
	2475465193456 [label=ReluBackward0]
	2475465193504 -> 2475465193456
	2475465193504 [label=NativeBatchNormBackward0]
	2475465193648 -> 2475465193504
	2475465193648 [label=ConvolutionBackward0]
	2475465193936 -> 2475465193648
	2475327182544 [label="downsample.layer1.0.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2475327182544 -> 2475465193936
	2475465193936 [label=AccumulateGrad]
	2475465193600 -> 2475465193504
	2475327182624 [label="downsample.layer1.1.weight
 (64)" fillcolor=lightblue]
	2475327182624 -> 2475465193600
	2475465193600 [label=AccumulateGrad]
	2475465193744 -> 2475465193504
	2475327182464 [label="downsample.layer1.1.bias
 (64)" fillcolor=lightblue]
	2475327182464 -> 2475465193744
	2475465193744 [label=AccumulateGrad]
	2475465193360 -> 2475465193072
	2475327181904 [label="downsample.layer1.4.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475327181904 -> 2475465193360
	2475465193360 [label=AccumulateGrad]
	2475465192928 -> 2475465193024
	2475327181984 [label="downsample.layer1.4.0.bn1.weight
 (64)" fillcolor=lightblue]
	2475327181984 -> 2475465192928
	2475465192928 [label=AccumulateGrad]
	2475465193168 -> 2475465193024
	2475327181824 [label="downsample.layer1.4.0.bn1.bias
 (64)" fillcolor=lightblue]
	2475327181824 -> 2475465193168
	2475465193168 [label=AccumulateGrad]
	2475465192832 -> 2475465192688
	2475327181264 [label="downsample.layer1.4.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475327181264 -> 2475465192832
	2475465192832 [label=AccumulateGrad]
	2475465192640 -> 2475465192448
	2475327181344 [label="downsample.layer1.4.0.bn2.weight
 (64)" fillcolor=lightblue]
	2475327181344 -> 2475465192640
	2475465192640 [label=AccumulateGrad]
	2475465192592 -> 2475465192448
	2475327181184 [label="downsample.layer1.4.0.bn2.bias
 (64)" fillcolor=lightblue]
	2475327181184 -> 2475465192592
	2475465192592 [label=AccumulateGrad]
	2475465192208 -> 2475465192400
	2475465192304 -> 2475465192016
	2475327180864 [label="downsample.layer1.4.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475327180864 -> 2475465192304
	2475465192304 [label=AccumulateGrad]
	2475465191872 -> 2475465191968
	2475327180624 [label="downsample.layer1.4.1.bn1.weight
 (64)" fillcolor=lightblue]
	2475327180624 -> 2475465191872
	2475465191872 [label=AccumulateGrad]
	2475465192112 -> 2475465191968
	2475327180784 [label="downsample.layer1.4.1.bn1.bias
 (64)" fillcolor=lightblue]
	2475327180784 -> 2475465192112
	2475465192112 [label=AccumulateGrad]
	2475465191776 -> 2475465191632
	2475327179904 [label="downsample.layer1.4.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475327179904 -> 2475465191776
	2475465191776 [label=AccumulateGrad]
	2475465191584 -> 2475465191392
	2475327180144 [label="downsample.layer1.4.1.bn2.weight
 (64)" fillcolor=lightblue]
	2475327180144 -> 2475465191584
	2475465191584 [label=AccumulateGrad]
	2475465191536 -> 2475465191392
	2475327179824 [label="downsample.layer1.4.1.bn2.bias
 (64)" fillcolor=lightblue]
	2475327179824 -> 2475465191536
	2475465191536 [label=AccumulateGrad]
	2475465191152 -> 2475465191344
	2475465191248 -> 2475465190960
	2475327184544 [label="downsample.layer2.0.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2475327184544 -> 2475465191248
	2475465191248 [label=AccumulateGrad]
	2475465190816 -> 2475465190912
	2475327177584 [label="downsample.layer2.0.0.bn1.weight
 (128)" fillcolor=lightblue]
	2475327177584 -> 2475465190816
	2475465190816 [label=AccumulateGrad]
	2475465191056 -> 2475465190912
	2475327184464 [label="downsample.layer2.0.0.bn1.bias
 (128)" fillcolor=lightblue]
	2475327184464 -> 2475465191056
	2475465191056 [label=AccumulateGrad]
	2475465190720 -> 2475465190576
	2475327154336 [label="downsample.layer2.0.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2475327154336 -> 2475465190720
	2475465190720 [label=AccumulateGrad]
	2475465190528 -> 2475465190336
	2475327145456 [label="downsample.layer2.0.0.bn2.weight
 (128)" fillcolor=lightblue]
	2475327145456 -> 2475465190528
	2475465190528 [label=AccumulateGrad]
	2475465190480 -> 2475465190336
	2475327154256 [label="downsample.layer2.0.0.bn2.bias
 (128)" fillcolor=lightblue]
	2475327154256 -> 2475465190480
	2475465190480 [label=AccumulateGrad]
	2475465190096 -> 2475465190288
	2475465190096 [label=NativeBatchNormBackward0]
	2475462385552 -> 2475465190096
	2475462385552 [label=ConvolutionBackward0]
	2475465180400 -> 2475462385552
	2475465191488 -> 2475462385552
	2475327179344 [label="downsample.layer2.0.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2475327179344 -> 2475465191488
	2475465191488 [label=AccumulateGrad]
	2475465190672 -> 2475465190096
	2475327179264 [label="downsample.layer2.0.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2475327179264 -> 2475465190672
	2475465190672 [label=AccumulateGrad]
	2475465190624 -> 2475465190096
	2475327179184 [label="downsample.layer2.0.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2475327179184 -> 2475465190624
	2475465190624 [label=AccumulateGrad]
	2475465190192 -> 2475465189904
	2475327144896 [label="downsample.layer2.0.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2475327144896 -> 2475465190192
	2475465190192 [label=AccumulateGrad]
	2475465189760 -> 2475465189856
	2475327154096 [label="downsample.layer2.0.1.bn1.weight
 (128)" fillcolor=lightblue]
	2475327154096 -> 2475465189760
	2475465189760 [label=AccumulateGrad]
	2475465190000 -> 2475465189856
	2475327145056 [label="downsample.layer2.0.1.bn1.bias
 (128)" fillcolor=lightblue]
	2475327145056 -> 2475465190000
	2475465190000 [label=AccumulateGrad]
	2475465189664 -> 2475465189520
	2475327153856 [label="downsample.layer2.0.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2475327153856 -> 2475465189664
	2475465189664 [label=AccumulateGrad]
	2475465189472 -> 2475465189280
	2475327144736 [label="downsample.layer2.0.1.bn2.weight
 (128)" fillcolor=lightblue]
	2475327144736 -> 2475465189472
	2475465189472 [label=AccumulateGrad]
	2475465189424 -> 2475465189280
	2475327153776 [label="downsample.layer2.0.1.bn2.bias
 (128)" fillcolor=lightblue]
	2475327153776 -> 2475465189424
	2475465189424 [label=AccumulateGrad]
	2475465189040 -> 2475465189232
	2475465189136 -> 2475465188848
	2475327159696 [label="downsample.layer3.0.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2475327159696 -> 2475465189136
	2475465189136 [label=AccumulateGrad]
	2475465188704 -> 2475465188800
	2475327159776 [label="downsample.layer3.0.0.bn1.weight
 (256)" fillcolor=lightblue]
	2475327159776 -> 2475465188704
	2475465188704 [label=AccumulateGrad]
	2475465188944 -> 2475465188800
	2475327153216 [label="downsample.layer3.0.0.bn1.bias
 (256)" fillcolor=lightblue]
	2475327153216 -> 2475465188944
	2475465188944 [label=AccumulateGrad]
	2475465188608 -> 2475465188464
	2475327153296 [label="downsample.layer3.0.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2475327153296 -> 2475465188608
	2475465188608 [label=AccumulateGrad]
	2475465188416 -> 2475465188224
	2475327153376 [label="downsample.layer3.0.0.bn2.weight
 (256)" fillcolor=lightblue]
	2475327153376 -> 2475465188416
	2475465188416 [label=AccumulateGrad]
	2475465188368 -> 2475465188224
	2475327159616 [label="downsample.layer3.0.0.bn2.bias
 (256)" fillcolor=lightblue]
	2475327159616 -> 2475465188368
	2475465188368 [label=AccumulateGrad]
	2475465187984 -> 2475465188176
	2475465187984 [label=NativeBatchNormBackward0]
	2475465189184 -> 2475465187984
	2475465189184 [label=ConvolutionBackward0]
	2475465181984 -> 2475465189184
	2475465189088 -> 2475465189184
	2475327153696 [label="downsample.layer3.0.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2475327153696 -> 2475465189088
	2475465189088 [label=AccumulateGrad]
	2475465188752 -> 2475465187984
	2475327153616 [label="downsample.layer3.0.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2475327153616 -> 2475465188752
	2475465188752 [label=AccumulateGrad]
	2475465188512 -> 2475465187984
	2475327160096 [label="downsample.layer3.0.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2475327160096 -> 2475465188512
	2475465188512 [label=AccumulateGrad]
	2475465188080 -> 2475465187792
	2475327152896 [label="downsample.layer3.0.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2475327152896 -> 2475465188080
	2475465188080 [label=AccumulateGrad]
	2475465187648 -> 2475465187744
	2475327159376 [label="downsample.layer3.0.1.bn1.weight
 (256)" fillcolor=lightblue]
	2475327159376 -> 2475465187648
	2475465187648 [label=AccumulateGrad]
	2475465187888 -> 2475465187744
	2475327152816 [label="downsample.layer3.0.1.bn1.bias
 (256)" fillcolor=lightblue]
	2475327152816 -> 2475465187888
	2475465187888 [label=AccumulateGrad]
	2475465187552 -> 2475465187408
	2475327152576 [label="downsample.layer3.0.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2475327152576 -> 2475465187552
	2475465187552 [label=AccumulateGrad]
	2475465187360 -> 2475465187168
	2475327159056 [label="downsample.layer3.0.1.bn2.weight
 (256)" fillcolor=lightblue]
	2475327159056 -> 2475465187360
	2475465187360 [label=AccumulateGrad]
	2475465187312 -> 2475465187168
	2475327152496 [label="downsample.layer3.0.1.bn2.bias
 (256)" fillcolor=lightblue]
	2475327152496 -> 2475465187312
	2475465187312 [label=AccumulateGrad]
	2475465186928 -> 2475465187120
	2475465187024 -> 2475465186832
	2475327151856 [label="downsample.layer4.0.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2475327151856 -> 2475465187024
	2475465187024 [label=AccumulateGrad]
	2475465186784 -> 2475465186736
	2475327151936 [label="downsample.layer4.0.0.bn1.weight
 (512)" fillcolor=lightblue]
	2475327151936 -> 2475465186784
	2475465186784 [label=AccumulateGrad]
	2475465186640 -> 2475465186736
	2475327158336 [label="downsample.layer4.0.0.bn1.bias
 (512)" fillcolor=lightblue]
	2475327158336 -> 2475465186640
	2475465186640 [label=AccumulateGrad]
	2475465186544 -> 2475465186400
	2475327151536 [label="downsample.layer4.0.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2475327151536 -> 2475465186544
	2475465186544 [label=AccumulateGrad]
	2475465186352 -> 2475465186256
	2475327151616 [label="downsample.layer4.0.0.bn2.weight
 (512)" fillcolor=lightblue]
	2475327151616 -> 2475465186352
	2475465186352 [label=AccumulateGrad]
	2475465186304 -> 2475465186256
	2475327158016 [label="downsample.layer4.0.0.bn2.bias
 (512)" fillcolor=lightblue]
	2475327158016 -> 2475465186304
	2475465186304 [label=AccumulateGrad]
	2475465186208 -> 2475465186160
	2475465186208 [label=NativeBatchNormBackward0]
	2475465187072 -> 2475465186208
	2475465187072 [label=ConvolutionBackward0]
	2475465183568 -> 2475465187072
	2475465186976 -> 2475465187072
	2475327158736 [label="downsample.layer4.0.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2475327158736 -> 2475465186976
	2475465186976 [label=AccumulateGrad]
	2475465186688 -> 2475465186208
	2475327152256 [label="downsample.layer4.0.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2475327152256 -> 2475465186688
	2475465186688 [label=AccumulateGrad]
	2475465186448 -> 2475465186208
	2475327152176 [label="downsample.layer4.0.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2475327152176 -> 2475465186448
	2475465186448 [label=AccumulateGrad]
	2475465186064 -> 2475465185872
	2475327151296 [label="downsample.layer4.0.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2475327151296 -> 2475465186064
	2475465186064 [label=AccumulateGrad]
	2475465185824 -> 2475465185776
	2475327157776 [label="downsample.layer4.0.1.bn1.weight
 (512)" fillcolor=lightblue]
	2475327157776 -> 2475465185824
	2475465185824 [label=AccumulateGrad]
	2475465185680 -> 2475465185776
	2475327151216 [label="downsample.layer4.0.1.bn1.bias
 (512)" fillcolor=lightblue]
	2475327151216 -> 2475465185680
	2475465185680 [label=AccumulateGrad]
	2475465185584 -> 2475465185440
	2475327150816 [label="downsample.layer4.0.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2475327150816 -> 2475465185584
	2475465185584 [label=AccumulateGrad]
	2475465185392 -> 2475465185296
	2475327157296 [label="downsample.layer4.0.1.bn2.weight
 (512)" fillcolor=lightblue]
	2475327157296 -> 2475465185392
	2475465185392 [label=AccumulateGrad]
	2475465185344 -> 2475465185296
	2475327150736 [label="downsample.layer4.0.1.bn2.bias
 (512)" fillcolor=lightblue]
	2475327150736 -> 2475465185344
	2475465185344 [label=AccumulateGrad]
	2475465185248 -> 2475465185200
	2475465184960 -> 2475465184816
	2475327150576 [label="up1.convT.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2475327150576 -> 2475465184960
	2475465184960 [label=AccumulateGrad]
	2475465184912 -> 2475465184816
	2475327157056 [label="up1.convT.bias
 (256)" fillcolor=lightblue]
	2475327157056 -> 2475465184912
	2475465184912 [label=AccumulateGrad]
	2475465184768 -> 2475465184720
	2475327156976 [label="up1.bn.weight
 (256)" fillcolor=lightblue]
	2475327156976 -> 2475465184768
	2475465184768 [label=AccumulateGrad]
	2475465184624 -> 2475465184720
	2475327150496 [label="up1.bn.bias
 (256)" fillcolor=lightblue]
	2475327150496 -> 2475465184624
	2475465184624 [label=AccumulateGrad]
	2475465184480 -> 2475465184336
	2475327150336 [label="up1.conv.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2475327150336 -> 2475465184480
	2475465184480 [label=AccumulateGrad]
	2475465184432 -> 2475465184336
	2475327150256 [label="up1.conv.conv1.bias
 (256)" fillcolor=lightblue]
	2475327150256 -> 2475465184432
	2475465184432 [label=AccumulateGrad]
	2475465184288 -> 2475465184240
	2475327156576 [label="up1.conv.bn1.weight
 (256)" fillcolor=lightblue]
	2475327156576 -> 2475465184288
	2475465184288 [label=AccumulateGrad]
	2475465184144 -> 2475465184240
	2475327156496 [label="up1.conv.bn1.bias
 (256)" fillcolor=lightblue]
	2475327156496 -> 2475465184144
	2475465184144 [label=AccumulateGrad]
	2475465184000 -> 2475465183856
	2475327149776 [label="up1.conv.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2475327149776 -> 2475465184000
	2475465184000 [label=AccumulateGrad]
	2475465183952 -> 2475465183856
	2475327156256 [label="up1.conv.conv2.bias
 (256)" fillcolor=lightblue]
	2475327156256 -> 2475465183952
	2475465183952 [label=AccumulateGrad]
	2475465183808 -> 2475465183760
	2475327156176 [label="up1.conv.bn2.weight
 (256)" fillcolor=lightblue]
	2475327156176 -> 2475465183808
	2475465183808 [label=AccumulateGrad]
	2475465183664 -> 2475465183760
	2475327149696 [label="up1.conv.bn2.bias
 (256)" fillcolor=lightblue]
	2475327149696 -> 2475465183664
	2475465183664 [label=AccumulateGrad]
	2475465183568 -> 2475465183424
	2475465183376 -> 2475465183232
	2475327155936 [label="up2.convT.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2475327155936 -> 2475465183376
	2475465183376 [label=AccumulateGrad]
	2475465183328 -> 2475465183232
	2475327155856 [label="up2.convT.bias
 (128)" fillcolor=lightblue]
	2475327155856 -> 2475465183328
	2475465183328 [label=AccumulateGrad]
	2475465183184 -> 2475465183136
	2475327149376 [label="up2.bn.weight
 (128)" fillcolor=lightblue]
	2475327149376 -> 2475465183184
	2475465183184 [label=AccumulateGrad]
	2475465183040 -> 2475465183136
	2475327149296 [label="up2.bn.bias
 (128)" fillcolor=lightblue]
	2475327149296 -> 2475465183040
	2475465183040 [label=AccumulateGrad]
	2475465182896 -> 2475465182752
	2475327156656 [label="up2.conv.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2475327156656 -> 2475465182896
	2475465182896 [label=AccumulateGrad]
	2475465182848 -> 2475465182752
	2475327150176 [label="up2.conv.conv1.bias
 (128)" fillcolor=lightblue]
	2475327150176 -> 2475465182848
	2475465182848 [label=AccumulateGrad]
	2475465182704 -> 2475465182656
	2475327150096 [label="up2.conv.bn1.weight
 (128)" fillcolor=lightblue]
	2475327150096 -> 2475465182704
	2475465182704 [label=AccumulateGrad]
	2475465182560 -> 2475465182656
	2475327145616 [label="up2.conv.bn1.bias
 (128)" fillcolor=lightblue]
	2475327145616 -> 2475465182560
	2475465182560 [label=AccumulateGrad]
	2475465182416 -> 2475465182272
	2475464744528 [label="up2.conv.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2475464744528 -> 2475465182416
	2475465182416 [label=AccumulateGrad]
	2475465182368 -> 2475465182272
	2475464744608 [label="up2.conv.conv2.bias
 (128)" fillcolor=lightblue]
	2475464744608 -> 2475465182368
	2475465182368 [label=AccumulateGrad]
	2475465182224 -> 2475465182176
	2475464744688 [label="up2.conv.bn2.weight
 (128)" fillcolor=lightblue]
	2475464744688 -> 2475465182224
	2475465182224 [label=AccumulateGrad]
	2475465182080 -> 2475465182176
	2475464744768 [label="up2.conv.bn2.bias
 (128)" fillcolor=lightblue]
	2475464744768 -> 2475465182080
	2475465182080 [label=AccumulateGrad]
	2475465181984 -> 2475465181840
	2475465181792 -> 2475465181648
	2475464745248 [label="up3.convT.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2475464745248 -> 2475465181792
	2475465181792 [label=AccumulateGrad]
	2475465181744 -> 2475465181648
	2475464745328 [label="up3.convT.bias
 (64)" fillcolor=lightblue]
	2475464745328 -> 2475465181744
	2475465181744 [label=AccumulateGrad]
	2475465181600 -> 2475465181552
	2475464745408 [label="up3.bn.weight
 (64)" fillcolor=lightblue]
	2475464745408 -> 2475465181600
	2475465181600 [label=AccumulateGrad]
	2475465181456 -> 2475465181552
	2475464745488 [label="up3.bn.bias
 (64)" fillcolor=lightblue]
	2475464745488 -> 2475465181456
	2475465181456 [label=AccumulateGrad]
	2475465181312 -> 2475465181168
	2475464745968 [label="up3.conv.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464745968 -> 2475465181312
	2475465181312 [label=AccumulateGrad]
	2475465181264 -> 2475465181168
	2475464746048 [label="up3.conv.conv1.bias
 (64)" fillcolor=lightblue]
	2475464746048 -> 2475465181264
	2475465181264 [label=AccumulateGrad]
	2475465181120 -> 2475465181072
	2475464746128 [label="up3.conv.bn1.weight
 (64)" fillcolor=lightblue]
	2475464746128 -> 2475465181120
	2475465181120 [label=AccumulateGrad]
	2475465180976 -> 2475465181072
	2475464746208 [label="up3.conv.bn1.bias
 (64)" fillcolor=lightblue]
	2475464746208 -> 2475465180976
	2475465180976 [label=AccumulateGrad]
	2475465180832 -> 2475465180688
	2475464746688 [label="up3.conv.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464746688 -> 2475465180832
	2475465180832 [label=AccumulateGrad]
	2475465180784 -> 2475465180688
	2475464746768 [label="up3.conv.conv2.bias
 (64)" fillcolor=lightblue]
	2475464746768 -> 2475465180784
	2475465180784 [label=AccumulateGrad]
	2475465180640 -> 2475465180592
	2475464746848 [label="up3.conv.bn2.weight
 (64)" fillcolor=lightblue]
	2475464746848 -> 2475465180640
	2475465180640 [label=AccumulateGrad]
	2475465180496 -> 2475465180592
	2475464746928 [label="up3.conv.bn2.bias
 (64)" fillcolor=lightblue]
	2475464746928 -> 2475465180496
	2475465180496 [label=AccumulateGrad]
	2475465180400 -> 2475465180256
	2475465180208 -> 2475465180064
	2475464747408 [label="up4.convT.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464747408 -> 2475465180208
	2475465180208 [label=AccumulateGrad]
	2475465180160 -> 2475465180064
	2475464747488 [label="up4.convT.bias
 (64)" fillcolor=lightblue]
	2475464747488 -> 2475465180160
	2475465180160 [label=AccumulateGrad]
	2475465180016 -> 2475465179968
	2475464747568 [label="up4.bn.weight
 (64)" fillcolor=lightblue]
	2475464747568 -> 2475465180016
	2475465180016 [label=AccumulateGrad]
	2475465179872 -> 2475465179968
	2475464747648 [label="up4.bn.bias
 (64)" fillcolor=lightblue]
	2475464747648 -> 2475465179872
	2475465179872 [label=AccumulateGrad]
	2475465179728 -> 2475465179584
	2475464748128 [label="up4.conv.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464748128 -> 2475465179728
	2475465179728 [label=AccumulateGrad]
	2475465179680 -> 2475465179584
	2475464748208 [label="up4.conv.conv1.bias
 (64)" fillcolor=lightblue]
	2475464748208 -> 2475465179680
	2475465179680 [label=AccumulateGrad]
	2475465179536 -> 2475465179488
	2475464748288 [label="up4.conv.bn1.weight
 (64)" fillcolor=lightblue]
	2475464748288 -> 2475465179536
	2475465179536 [label=AccumulateGrad]
	2475465179392 -> 2475465179488
	2475464748368 [label="up4.conv.bn1.bias
 (64)" fillcolor=lightblue]
	2475464748368 -> 2475465179392
	2475465179392 [label=AccumulateGrad]
	2475465179248 -> 2475464654752
	2475464748768 [label="up4.conv.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464748768 -> 2475465179248
	2475465179248 [label=AccumulateGrad]
	2475465179200 -> 2475464654752
	2475464748848 [label="up4.conv.conv2.bias
 (64)" fillcolor=lightblue]
	2475464748848 -> 2475465179200
	2475465179200 [label=AccumulateGrad]
	2475464654704 -> 2475464654656
	2475464748928 [label="up4.conv.bn2.weight
 (64)" fillcolor=lightblue]
	2475464748928 -> 2475464654704
	2475464654704 [label=AccumulateGrad]
	2475464654560 -> 2475464654656
	2475464749008 [label="up4.conv.bn2.bias
 (64)" fillcolor=lightblue]
	2475464749008 -> 2475464654560
	2475464654560 [label=AccumulateGrad]
	2475464654416 -> 2475464654272
	2475464751648 [label="fpn4.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464751648 -> 2475464654416
	2475464654416 [label=AccumulateGrad]
	2475464654368 -> 2475464654272
	2475464751728 [label="fpn4.conv1.bias
 (64)" fillcolor=lightblue]
	2475464751728 -> 2475464654368
	2475464654368 [label=AccumulateGrad]
	2475464654224 -> 2475464654176
	2475464751808 [label="fpn4.bn.weight
 (64)" fillcolor=lightblue]
	2475464751808 -> 2475464654224
	2475464654224 [label=AccumulateGrad]
	2475464653984 -> 2475464654176
	2475464751888 [label="fpn4.bn.bias
 (64)" fillcolor=lightblue]
	2475464751888 -> 2475464653984
	2475464653984 [label=AccumulateGrad]
	2475464653888 -> 2475464653696
	2475464653888 [label=UpsampleNearest2DBackward0]
	2475464654320 -> 2475464653888
	2475464654320 [label=ReluBackward0]
	2475464654800 -> 2475464654320
	2475464654800 [label=NativeBatchNormBackward0]
	2475464654512 -> 2475464654800
	2475464654512 [label=ConvolutionBackward0]
	2475465180256 -> 2475464654512
	2475465179920 -> 2475464654512
	2475464750928 [label="fpn3.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464750928 -> 2475465179920
	2475465179920 [label=AccumulateGrad]
	2475465180112 -> 2475464654512
	2475464751008 [label="fpn3.conv1.bias
 (64)" fillcolor=lightblue]
	2475464751008 -> 2475465180112
	2475465180112 [label=AccumulateGrad]
	2475464654032 -> 2475464654800
	2475464751088 [label="fpn3.bn.weight
 (64)" fillcolor=lightblue]
	2475464751088 -> 2475464654032
	2475464654032 [label=AccumulateGrad]
	2475465179632 -> 2475464654800
	2475464751168 [label="fpn3.bn.bias
 (64)" fillcolor=lightblue]
	2475464751168 -> 2475465179632
	2475465179632 [label=AccumulateGrad]
	2475464653840 -> 2475464653696
	2475464653840 [label=UpsampleNearest2DBackward0]
	2475464654608 -> 2475464653840
	2475464654608 [label=ReluBackward0]
	2475465179824 -> 2475464654608
	2475465179824 [label=NativeBatchNormBackward0]
	2475465180352 -> 2475465179824
	2475465180352 [label=ConvolutionBackward0]
	2475465181840 -> 2475465180352
	2475465181024 -> 2475465180352
	2475464750208 [label="fpn2.conv1.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2475464750208 -> 2475465181024
	2475465181024 [label=AccumulateGrad]
	2475465181216 -> 2475465180352
	2475464750288 [label="fpn2.conv1.bias
 (64)" fillcolor=lightblue]
	2475464750288 -> 2475465181216
	2475465181216 [label=AccumulateGrad]
	2475465179344 -> 2475465179824
	2475464750368 [label="fpn2.bn.weight
 (64)" fillcolor=lightblue]
	2475464750368 -> 2475465179344
	2475465179344 [label=AccumulateGrad]
	2475465179440 -> 2475465179824
	2475464750448 [label="fpn2.bn.bias
 (64)" fillcolor=lightblue]
	2475464750448 -> 2475465179440
	2475465179440 [label=AccumulateGrad]
	2475464653792 -> 2475464653696
	2475464653792 [label=UpsampleNearest2DBackward0]
	2475464654128 -> 2475464653792
	2475464654128 [label=ReluBackward0]
	2475465180928 -> 2475464654128
	2475465180928 [label=NativeBatchNormBackward0]
	2475465181504 -> 2475465180928
	2475465181504 [label=ConvolutionBackward0]
	2475465183424 -> 2475465181504
	2475465181888 -> 2475465181504
	2475464749488 [label="fpn1.conv1.weight
 (64, 256, 3, 3)" fillcolor=lightblue]
	2475464749488 -> 2475465181888
	2475465181888 [label=AccumulateGrad]
	2475465181936 -> 2475465181504
	2475464749568 [label="fpn1.conv1.bias
 (64)" fillcolor=lightblue]
	2475464749568 -> 2475465181936
	2475465181936 [label=AccumulateGrad]
	2475465180736 -> 2475465180928
	2475464749648 [label="fpn1.bn.weight
 (64)" fillcolor=lightblue]
	2475464749648 -> 2475465180736
	2475465180736 [label=AccumulateGrad]
	2475465180544 -> 2475465180928
	2475464749728 [label="fpn1.bn.bias
 (64)" fillcolor=lightblue]
	2475464749728 -> 2475465180544
	2475465180544 [label=AccumulateGrad]
	2475464653648 -> 2475464653504
	2475464752288 [label="up5.convT.weight
 (256, 64, 2, 2)" fillcolor=lightblue]
	2475464752288 -> 2475464653648
	2475464653648 [label=AccumulateGrad]
	2475464653600 -> 2475464653504
	2475464752368 [label="up5.convT.bias
 (64)" fillcolor=lightblue]
	2475464752368 -> 2475464653600
	2475464653600 [label=AccumulateGrad]
	2475464653456 -> 2475464653408
	2475464752448 [label="up5.bn.weight
 (64)" fillcolor=lightblue]
	2475464752448 -> 2475464653456
	2475464653456 [label=AccumulateGrad]
	2475464653312 -> 2475464653408
	2475464752528 [label="up5.bn.bias
 (64)" fillcolor=lightblue]
	2475464752528 -> 2475464653312
	2475464653312 [label=AccumulateGrad]
	2475464653168 -> 2475464653024
	2475464753008 [label="up5.conv.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475464753008 -> 2475464653168
	2475464653168 [label=AccumulateGrad]
	2475464653120 -> 2475464653024
	2475464753088 [label="up5.conv.conv1.bias
 (64)" fillcolor=lightblue]
	2475464753088 -> 2475464653120
	2475464653120 [label=AccumulateGrad]
	2475464652976 -> 2475464652928
	2475465080912 [label="up5.conv.bn1.weight
 (64)" fillcolor=lightblue]
	2475465080912 -> 2475464652976
	2475464652976 [label=AccumulateGrad]
	2475464652832 -> 2475464652928
	2475465080992 [label="up5.conv.bn1.bias
 (64)" fillcolor=lightblue]
	2475465080992 -> 2475464652832
	2475464652832 [label=AccumulateGrad]
	2475464652688 -> 2475464652544
	2475465081472 [label="up5.conv.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2475465081472 -> 2475464652688
	2475464652688 [label=AccumulateGrad]
	2475464652640 -> 2475464652544
	2475465081552 [label="up5.conv.conv2.bias
 (64)" fillcolor=lightblue]
	2475465081552 -> 2475464652640
	2475464652640 [label=AccumulateGrad]
	2475464652496 -> 2475464652448
	2475465081632 [label="up5.conv.bn2.weight
 (64)" fillcolor=lightblue]
	2475465081632 -> 2475464652496
	2475464652496 [label=AccumulateGrad]
	2475464652352 -> 2475464652448
	2475465081712 [label="up5.conv.bn2.bias
 (64)" fillcolor=lightblue]
	2475465081712 -> 2475464652352
	2475464652352 [label=AccumulateGrad]
	2475464652160 -> 2475464652304
	2475465082112 [label="conv1.conv1.weight
 (32, 64, 3, 3)" fillcolor=lightblue]
	2475465082112 -> 2475464652160
	2475464652160 [label=AccumulateGrad]
	2475464652208 -> 2475464652304
	2475465082192 [label="conv1.conv1.bias
 (32)" fillcolor=lightblue]
	2475465082192 -> 2475464652208
	2475464652208 [label=AccumulateGrad]
	2475464652016 -> 2475464651968
	2475465082272 [label="conv1.bn1.weight
 (32)" fillcolor=lightblue]
	2475465082272 -> 2475464652016
	2475464652016 [label=AccumulateGrad]
	2475464651872 -> 2475464651968
	2475465082352 [label="conv1.bn1.bias
 (32)" fillcolor=lightblue]
	2475465082352 -> 2475464651872
	2475464651872 [label=AccumulateGrad]
	2475464651728 -> 2475464651440
	2475465082832 [label="conv1.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2475465082832 -> 2475464651728
	2475464651728 [label=AccumulateGrad]
	2475464651680 -> 2475464651440
	2475465082912 [label="conv1.conv2.bias
 (32)" fillcolor=lightblue]
	2475465082912 -> 2475464651680
	2475464651680 [label=AccumulateGrad]
	2475464651536 -> 2475464651584
	2475465082992 [label="conv1.bn2.weight
 (32)" fillcolor=lightblue]
	2475465082992 -> 2475464651536
	2475464651536 [label=AccumulateGrad]
	2475464651488 -> 2475464651584
	2475465083072 [label="conv1.bn2.bias
 (32)" fillcolor=lightblue]
	2475465083072 -> 2475464651488
	2475464651488 [label=AccumulateGrad]
	2475464651296 -> 2475464651344
	2475465083552 [label="conv2.weight
 (1, 32, 1, 1)" fillcolor=lightblue]
	2475465083552 -> 2475464651296
	2475464651296 [label=AccumulateGrad]
	2475464651392 -> 2475464651344
	2475465083632 [label="conv2.bias
 (1)" fillcolor=lightblue]
	2475465083632 -> 2475464651392
	2475464651392 [label=AccumulateGrad]
	2475464651344 -> 2475465089872
}
